{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b87548b",
   "metadata": {},
   "source": [
    "# PREPARATION OF PRODUCTION CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b77dab",
   "metadata": {},
   "source": [
    "Goal: Prepare the final, production-ready pipeline for the RUL prediction model.\n",
    "\n",
    "Key idea:\n",
    "- Pandas-based operations are done before the pipeline (data cleaning, and changing structure).\n",
    "- scikit-learn transformations are done inside the pipeline (for modeling). \n",
    "\n",
    "\n",
    "Our approach at this stage is:\n",
    "\n",
    "**1. Load the raw dataset.** Read original data (no preprocessing applied). \n",
    "\n",
    "\n",
    "**2. Transformations in the structure (outside the pipeline, with pandas)**\n",
    "- Correct column names\n",
    "- Remove duplicates and nulls (not needed in this project)\n",
    "- Restrict the dataset to final selected variables\n",
    "- Create the target variable (RUL) (and declare X and y)\n",
    "\n",
    "**3. Create the pipeline and include transformations in the data (with Sklearn)**, such as:\n",
    "- imputations, encodings, and scalings using scikit-learn transformers (only standard scaling in this project)\n",
    "\n",
    "**4. Integrate the model within the pipeline**\n",
    "\n",
    " \n",
    "**5. Save the final execution-ready pipeline and store it for retraining or production use**\n",
    "\n",
    "After this, there will be only 2 notebooks more, which are actually 2 scripts, in which we paste what we preparated in this notebook:\n",
    "- 08_Retraining Code\n",
    "- 09_Execution Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24546214",
   "metadata": {},
   "source": [
    "## IMPORT LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "53bbdfa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cloudpickle  # alternative to Pickle that also allows saving custom Pandas functions\n",
    "\n",
    "#Enable fast autocomplete\n",
    "%config IPCompleter.greedy=True\n",
    "\n",
    "#Libraries needed for any project\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "#Specific libraries for this example project template:\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1809fa9b",
   "metadata": {},
   "source": [
    "## IMPORT DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859bb27d",
   "metadata": {},
   "source": [
    "### Import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "18eb4cd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6c/byy38myn1t50449jbzfjs1_c0000gn/T/ipykernel_24561/2843707148.py:12: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  df = pd.read_csv(train_path, delim_whitespace=True, header=None)\n"
     ]
    }
   ],
   "source": [
    "# Project path\n",
    "PROJECT_PATH = '/Users/rober/cmapss-rul-prediction'\n",
    "\n",
    "# Dataset selection (FD001, FD002, FD003, or FD004)\n",
    "dataset_id = 'FD001'\n",
    "\n",
    "# Data paths\n",
    "path = PROJECT_PATH + '/02_Data/01_Raw/'\n",
    "train_path = path + f'train_{dataset_id}.txt'\n",
    "\n",
    "# Load datasets\n",
    "df = pd.read_csv(train_path, delim_whitespace=True, header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109af02a",
   "metadata": {},
   "source": [
    "### Select only selected variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5bbce2",
   "metadata": {},
   "source": [
    "#### Load list of selected variables (saved on previous notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "394c2b2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['time_in_cycles_ss',\n",
       " 'sensor_11_ss',\n",
       " 'sensor_4_ss',\n",
       " 'sensor_12_ss',\n",
       " 'sensor_7_ss',\n",
       " 'sensor_15_ss',\n",
       " 'sensor_21_ss',\n",
       " 'sensor_20_ss']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_variables_path = PROJECT_PATH + '/05_Results/' + 'selected_variables.pickle'\n",
    "\n",
    "pd.read_pickle(selected_variables_path).sort_index().values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d340e0",
   "metadata": {},
   "source": [
    "#### List of selected variables without sufixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "69c5a812",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_variables = ['time_in_cycles',\n",
    "                     'sensor_11',\n",
    "                     'sensor_4',\n",
    "                     'sensor_12',\n",
    "                     'sensor_7',\n",
    "                     'sensor_15',\n",
    "                     'sensor_21',\n",
    "                     'sensor_20']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9022829e",
   "metadata": {},
   "source": [
    "#### Transformations in selected variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e264ea28",
   "metadata": {},
   "source": [
    "| |  | time_in_cycles | sensor_11 | sensor_4 | sensor_12 | sensor_7 | sensor_15 | sensor_21 | sensor_20 |\n",
    "| --------------- | ------ | -------------- | --------- | -------- | --------- | -------- | --------- | --------- | --------- |\n",
    "| NAMES           | MANUAL | X              | X         | X        | X         | X        | X         | X         | X         |\n",
    "| TYPES           |        |                | X         | X        | X         | X        | X         | X         | X         |\n",
    "| RESCALING       | SS     | X              | X         | X        | X         | X        | X         | X         | X         |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d0f577",
   "metadata": {},
   "source": [
    "## STRUCTURAL TRANSFORMATIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e0a70d",
   "metadata": {},
   "source": [
    "1. Correct names (original columns have jut numbers as names)\n",
    "2. Create the target (calculate RUL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df48d39",
   "metadata": {},
   "source": [
    "### Correct names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "df94e255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name columns dynamically\n",
    "\n",
    "n_cols = df.shape[1] # this n_cols is valid for 'test' also\n",
    "n_sensors = n_cols - 5 # first 4 columns are not sensors\n",
    "\n",
    "columns = (\n",
    "    ['unit_number', 'time_in_cycles'] +\n",
    "    [f'op_setting_{i}' for i in range(1, 4)] +\n",
    "    [f'sensor_{i}' for i in range(1, n_sensors + 1)]\n",
    ") # concatenate names to create the whole list of column names ('columns')\n",
    "\n",
    "df.columns = columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820bbcde",
   "metadata": {},
   "source": [
    "### Create the target and assign X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "11a54905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the last cycle for each engine\n",
    "rul_per_unit = df.groupby('unit_number')['time_in_cycles'].max().reset_index()\n",
    "rul_per_unit.columns = ['unit_number', 'max_cycle']\n",
    "\n",
    "# Merge back to the main dataframe\n",
    "df = df.merge(rul_per_unit, on='unit_number', how='left')\n",
    "\n",
    "# Calculate RUL\n",
    "df['RUL'] = df['max_cycle'] - df['time_in_cycles']\n",
    "\n",
    "# Clean up (optional)\n",
    "df.drop(columns=['max_cycle'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f53fe7e",
   "metadata": {},
   "source": [
    "#### For X: index only selected variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b2a2942a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[selected_variables].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ba011a",
   "metadata": {},
   "source": [
    "#### For y: specify the target and create y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9fe09494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define target variable\n",
    "target = 'RUL'\n",
    "\n",
    "# Create y\n",
    "y = df[target].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923cb435",
   "metadata": {},
   "source": [
    "## CREATE THE PIPELINE (for data quality and transformations; not mandatory but efficient) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d202414",
   "metadata": {},
   "source": [
    "### Instance data quality function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af10c7e1",
   "metadata": {},
   "source": [
    "#### Create the function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01cf89e2",
   "metadata": {},
   "source": [
    "The only data quality process we did with selected variables was coverting types to float."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "66d62fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_quality(df):\n",
    "    \n",
    "    # Make a copy to avoid mutating original\n",
    "    temp = df.copy()            \n",
    "    \n",
    "    # All sensors to float to unify types\n",
    "    sensor_cols = [col for col in df.columns if 'sensor_' in col]\n",
    "    df[sensor_cols] = df[sensor_cols].astype(float)\n",
    "    \n",
    "    return(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772cd664",
   "metadata": {},
   "source": [
    "#### Convert the function in a transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6e2adfee",
   "metadata": {},
   "outputs": [],
   "source": [
    "do_data_quality = FunctionTransformer(data_quality)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7975d1",
   "metadata": {},
   "source": [
    "### Instance transformations in variables (feature engineering)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2b8f53",
   "metadata": {},
   "source": [
    "The only transformation we did with selected variables was standard scalling (rescaling) with all of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "eb48fec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = StandardScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6809643",
   "metadata": {},
   "source": [
    "### Create the preprocessing pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a91992",
   "metadata": {},
   "source": [
    "#### Create the column transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "028acab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = make_column_transformer(\n",
    "    (ss, selected_variables),\n",
    "    remainder='drop')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ec225a",
   "metadata": {},
   "source": [
    "#### Create the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3a0fde6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_prepro = make_pipeline(do_data_quality, \n",
    "                            ct)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff42c5a",
   "metadata": {},
   "source": [
    "### Instance the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8d8ca4",
   "metadata": {},
   "source": [
    "#### Instance the algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1145e22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HistGradientBoostingRegressor(l2_regularization=0.5,\n",
    "                                      learning_rate=0.025,\n",
    "                                      max_depth=10, max_iter=200,\n",
    "                                      min_samples_leaf=500,\n",
    "                                      scoring='neg_mean_absolute_percentage_error')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e145572",
   "metadata": {},
   "source": [
    "#### Build the final training pipeline (not yet trained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f6b329eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_training = make_pipeline(pipe_prepro, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472e2ad2",
   "metadata": {},
   "source": [
    "#### Save the final training pipeline (not yet trained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3ec0df9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_training_name = 'pipe_training.pickle'\n",
    "\n",
    "PIPE_TRAINING_PATH = PROJECT_PATH + '/04_Models/' + pipe_training_name\n",
    "\n",
    "with open(PIPE_TRAINING_PATH, mode='wb') as file:\n",
    "   cloudpickle.dump(pipe_training, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2651db",
   "metadata": {},
   "source": [
    "#### Train the execution pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a54d18cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_execution = pipe_training.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59a201c",
   "metadata": {},
   "source": [
    "## SAVE THE PIPELINE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f8c02e",
   "metadata": {},
   "source": [
    "### Save the execution pipeline (already trained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3769e7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_execution_name = 'pipe_execution.pickle'\n",
    "\n",
    "PIPE_EXECUTION_PATH = PROJECT_PATH + '/04_Models/' + pipe_execution_name\n",
    "\n",
    "with open(PIPE_EXECUTION_PATH, mode='wb') as file:\n",
    "   cloudpickle.dump(pipe_execution, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994b01f5",
   "metadata": {},
   "source": [
    "## Recap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5e23ef",
   "metadata": {},
   "source": [
    "We have saved:\n",
    "\n",
    "- **pipe_training**: the untrained pipeline, in case we want to retrain it in the future.\n",
    "- **pipe_execution**: the trained pipeline (already fitted), which we will later use to make predictions.\n",
    "\n",
    "From this point on, we will generate two scripts:\n",
    "\n",
    "- **Retraining**: \n",
    "\n",
    "    - Models gradually lose predictive power over time (data drifts, the market evolves...)\n",
    "    - There is no fixed rule for how often to retrain (e.g., insurance or energy companies may retrain every 5 years, while in digital advertising it can be every few seconds). \n",
    "    - Typically, retraining is triggered when predictive performance drops by 5–10%. \n",
    "    - We’ll keep this code ready, although the one we’ll actually use is the execution script. \n",
    "    \n",
    "\n",
    "- **Execution**: An engineer will deploy this script in a production environment (to run in batch mode, or via API, or as part of an app...)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
